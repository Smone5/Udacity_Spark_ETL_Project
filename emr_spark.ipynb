{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Amazon EMR Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d571f625ac4a53b329621a9011df0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1595500459354_0002</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import to_timestamp, from_unixtime, to_date\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1ec19b33a44387a51a4c972ae83c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    # get filepath to song data file\n",
    "    song_data = input_data+'song_data/*/*/*/*'\n",
    "    \n",
    "    # read song data file\n",
    "    song_schema = R([Fld(\"artist\", Str()),\n",
    "                     Fld(\"artist_id\", Str()),\n",
    "                     Fld(\"artist_latitude\", Dbl()),\n",
    "                     Fld(\"artist_location\", Str()),\n",
    "                     Fld(\"artist_longitude\", Dbl()),\n",
    "                     Fld(\"artist_name\", Str()),\n",
    "                     Fld(\"duration\", Dbl()),\n",
    "                     Fld(\"num_songs\", Int()),\n",
    "                     Fld(\"song_id\", Str()),\n",
    "                     Fld(\"title\", Str()),\n",
    "                     Fld(\"year\", Int()),  \n",
    "                     ])\n",
    "    print(\"Reading song data\")\n",
    "    df = spark.read.json(song_data, song_schema)\n",
    "\n",
    "    print(\"Creating songs table\")\n",
    "    # extract columns to create songs table\n",
    "    cols = ['song_id','title', 'artist_id', 'year','duration']\n",
    "    songs_table = df.select(cols)\n",
    "    songs_table = songs_table.drop_duplicates()\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table = songs_table.repartition('year','artist_id')\n",
    "    \n",
    "    songs_table_output = output_data+\"songs_table/songs_table.parquet\"\n",
    "    songs_table.write.parquet(songs_table_output)\n",
    "\n",
    "    print(\"Creating artist table\")\n",
    "    # extract columns to create artists table\n",
    "    cols = ['artist_id','artist_name as name', 'artist_location as location', 'artist_latitude as latitude','artist_longitude as longitude']\n",
    "    artist_table = df.selectExpr(cols)\n",
    "    artist_table = artist_table.drop_duplicates()\n",
    "    \n",
    "            \n",
    "    # write artists table to parquet files\n",
    "    artist_table_output = output_data+\"artist_table/artist_table.parquet\"\n",
    "    artist_table.write.parquet(artist_table_output)\n",
    "\n",
    "\n",
    "def process_log_data(spark, input_data, output_data):\n",
    "    # get filepath to log data file\n",
    "    \n",
    "    log_data = \"s3a://udacity-dend/log_data/*\"\n",
    "\n",
    "    # read log data file\n",
    "    log_schema = R([Fld(\"artist\", Str()),\n",
    "                     Fld(\"auth\", Str()),\n",
    "                     Fld(\"firstName\", Str()),\n",
    "                     Fld(\"gender\", Str()),\n",
    "                     Fld(\"itemInSession\", Int()),\n",
    "                     Fld(\"lastName\", Str()),\n",
    "                     Fld(\"length\", Dbl()),\n",
    "                     Fld(\"level\", Str()),\n",
    "                     Fld(\"location\", Str()),\n",
    "                     Fld(\"method\", Str()),\n",
    "                     Fld(\"page\", Str()),\n",
    "                     Fld(\"registration\", Str()),\n",
    "                     Fld(\"sessionId\", Int()),\n",
    "                     Fld(\"song\", Str()),\n",
    "                     Fld(\"status\", Int()),\n",
    "                     Fld(\"ts\", Str()),\n",
    "                     Fld(\"userAgent\", Str()),\n",
    "                     Fld(\"userId\", Str(), False)    \n",
    "                     ])\n",
    "    \n",
    "    print(\"Reading log data\")\n",
    "    df = spark.read.json(log_data, log_schema)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df.filter(df.page == 'NextSong')\n",
    "\n",
    "    print(\"Creating users table\")\n",
    "    # extract columns for users table \n",
    "    cols = ['userId as user_id','firstName as first_name', 'lastName as last_name', 'gender', 'level']\n",
    "    users_table = df.selectExpr(cols)\n",
    "    users_table = users_table.drop_duplicates()\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table_output = output_data+\"users_table/users_table.parquet\"\n",
    "    users_table.write.parquet(users_table_output)\n",
    "\n",
    "    print(\"Creating time table\")\n",
    "    \n",
    "    # create timestamp column from original timestamp column\n",
    "    def parse_time(line : str) -> str:\n",
    "        return(line[0:-3])\n",
    "    \n",
    "    parse_time_udf = udf(lambda epoch: parse_time(epoch), Str())\n",
    "    df = df.withColumn(\"start_time\", to_timestamp(from_unixtime(parse_time_udf(col(\"ts\")))))\n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "    parse_time_udf = udf(lambda epoch: parse_time(epoch), Str())\n",
    "    df = df.withColumn(\"date\", to_date(from_unixtime(parse_time_udf(col(\"ts\")))))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = df.select('start_time')\n",
    "    time_table = time_table.withColumn('hour', hour(time_table.start_time))\n",
    "    time_table = time_table.withColumn('day', dayofmonth(time_table.start_time))\n",
    "    time_table = time_table.withColumn('week', weekofyear(time_table.start_time))\n",
    "    time_table = time_table.withColumn('month', month(time_table.start_time))\n",
    "    time_table = time_table.withColumn('year', year(time_table.start_time))\n",
    "    time_table = time_table.withColumn(\"weekday\", date_format(df.start_time, \"EEEE\"))\n",
    "    time_table = time_table.drop_duplicates()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table = time_table.repartition('year','month')\n",
    "    \n",
    "    time_table_output = output_data+\"time_table/time_table.parquet\"\n",
    "    time_table.write.parquet(time_table_output)\n",
    "    \n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    song_data = input_data+'song_data/*/*/*/*'\n",
    "    \n",
    "    song_schema = R([Fld(\"artist\", Str()),\n",
    "                     Fld(\"artist_id\", Str()),\n",
    "                     Fld(\"artist_latitude\", Dbl()),\n",
    "                     Fld(\"artist_location\", Str()),\n",
    "                     Fld(\"artist_longitude\", Dbl()),\n",
    "                     Fld(\"artist_name\", Str()),\n",
    "                     Fld(\"duration\", Dbl()),\n",
    "                     Fld(\"num_songs\", Int()),\n",
    "                     Fld(\"song_id\", Str()),\n",
    "                     Fld(\"title\", Str()),\n",
    "                     Fld(\"year\", Int()),  \n",
    "                     ])\n",
    "    \n",
    "    print(\"Creating songplays table\")\n",
    "    song_df = spark.read.json(song_data, song_schema)\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table\n",
    "    df.createOrReplaceTempView(\"e\")\n",
    "    song_df.createOrReplaceTempView(\"song_stage\")\n",
    "    songplays_table = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        CAST(e.userId || e.sessionId || e.itemInSession as bigint) as songplay_id,\n",
    "        e.ts start_time,\n",
    "        CAST(e.userId as int) as user_id,\n",
    "        e.level as level,\n",
    "        st.song_id,\n",
    "        st.artist_id,\n",
    "        CAST(e.itemInSession as int) as session_id,\n",
    "        e.location as location,\n",
    "        e.userAgent as user_agent\n",
    "    FROM e\n",
    "    LEFT JOIN (select distinct title, artist_name, artist_id, duration, song_id from song_stage) as st\n",
    "        ON ( e.song = st.title and e.artist = st.artist_name and e.length = st.duration)\n",
    "    ORDER BY start_time ASC\n",
    "    \"\"\")\n",
    "    songplays_table = songplays_table.withColumn(\"start_time\", to_timestamp(from_unixtime(parse_time_udf(col(\"start_time\")))))\n",
    "    \n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table = songplays_table.repartition( year(songplays_table.start_time),month(songplays_table.start_time))\n",
    "    \n",
    "    songsplay_output = output_data+\"songplays_table/songplays_table.parquet\"\n",
    "    songplays_table.write.parquet(songsplay_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faadfdcc932b40d7ab103e7cba8106e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f9c7f021c50>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2974467511fb43c797ec346765469423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"s3a://sparkify-datawharehouse23/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6c4420413e4a39abe8ee2909e5032a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading song data\n",
      "Creating songs table\n",
      "Creating artist table"
     ]
    }
   ],
   "source": [
    "process_song_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96673ba67194a2fbedbe8be356d9a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading log data\n",
      "Creating users table\n",
      "Creating time table\n",
      "Creating songplays table"
     ]
    }
   ],
   "source": [
    "process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
